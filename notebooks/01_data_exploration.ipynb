{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2f7a09-6df4-49b6-861d-e3d0fa7a1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Setup Complete\n",
      "Python version: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "Pandas version: 2.3.2\n",
      "NumPy version: 2.2.6\n",
      "Available memory: 6.76 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Addding src to path for imports\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', '..', 'src'))\n",
    "\n",
    "# Configuring display and warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting plot style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment Setup Complete\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Available memory: {psutil.virtual_memory().available / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf0440d-01e0-4925-9fb0-12b9ce3d2fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  memory_limit_gb: 12\n",
      "  chunk_size: 10000\n",
      "  sample_size: 50000\n",
      "  random_state: 42\n",
      "\n",
      "Data files expected at:\n",
      "  CICFlowMeter: C:\\Users\\Ashutosh\\Documents\\Projects\\NIDS\\data\\raw\\CICFlowMeter_out.csv\n",
      "  Data: C:\\Users\\Ashutosh\\Documents\\Projects\\NIDS\\data\\raw\\Data.csv\n",
      "  Labels: C:\\Users\\Ashutosh\\Documents\\Projects\\NIDS\\data\\raw\\Label.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Defining Paths and Configuration\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "DATA_RAW_PATH = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED_PATH = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "RESULTS_PATH = PROJECT_ROOT / \"results\" / \"data_exploration\"\n",
    "\n",
    "# Ensuring directories exist\n",
    "DATA_PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset file paths\n",
    "CICFLOW_FILE = DATA_RAW_PATH / \"CICFlowMeter_out.csv\"\n",
    "DATA_FILE = DATA_RAW_PATH / \"Data.csv\"\n",
    "LABEL_FILE = DATA_RAW_PATH / \"Label.csv\"\n",
    "\n",
    "# Configuration for analysis\n",
    "ANALYSIS_CONFIG = {\n",
    "    'memory_limit_gb': 12,  # Conservative limit for 16GB system\n",
    "    'chunk_size': 10000,    # For chunked processing\n",
    "    'sample_size': 50000,   # For quick analysis\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in ANALYSIS_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nData files expected at:\")\n",
    "print(f\"  CICFlowMeter: {CICFLOW_FILE}\")\n",
    "print(f\"  Data: {DATA_FILE}\")\n",
    "print(f\"  Labels: {LABEL_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f20e4d-3bf5-49ba-b1e2-58be0e714404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== File System Analysis ===\n",
      "Analysis timestamp: 2025-09-28T15:38:53.794335\n",
      "\n",
      "System Information:\n",
      "  total_memory_gb: 15.35 GB\n",
      "  available_memory_gb: 6.80 GB\n",
      "  cpu_count: 16\n",
      "  disk_usage_gb: 76.44 GB\n",
      "\n",
      "Dataset Files:\n",
      "  CICFLOW:\n",
      "    Size: 1.78 GB (1825.6 MB)\n",
      "    Path: C:\\Users\\Ashutosh\\Documents\\Projects\\NIDS\\data\\raw\\CICFlowMeter_out.csv\n",
      "  DATA:\n",
      "    Size: 0.18 GB (187.2 MB)\n",
      "    Path: C:\\Users\\Ashutosh\\Documents\\Projects\\NIDS\\data\\raw\\Data.csv\n",
      "  LABELS:\n",
      "    Size: 0.00 GB (0.9 MB)\n",
      "    Path: C:\\Users\\Ashutosh\\Documents\\Projects\\NIDS\\data\\raw\\Label.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: File System Analysis\n",
    "def analyze_file_system():\n",
    "    \"\"\"Analyze the file system and dataset files\"\"\"\n",
    "    \n",
    "    analysis = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'files': {},\n",
    "        'system_info': {\n",
    "            'total_memory_gb': psutil.virtual_memory().total / (1024**3),\n",
    "            'available_memory_gb': psutil.virtual_memory().available / (1024**3),\n",
    "            'cpu_count': psutil.cpu_count(),\n",
    "            'disk_usage_gb': psutil.disk_usage('.').free / (1024**3)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Checking each file\n",
    "    files_to_check = {\n",
    "        'cicflow': CICFLOW_FILE,\n",
    "        'data': DATA_FILE,\n",
    "        'labels': LABEL_FILE\n",
    "    }\n",
    "    \n",
    "    for name, filepath in files_to_check.items():\n",
    "        if filepath.exists():\n",
    "            file_size = filepath.stat().st_size\n",
    "            analysis['files'][name] = {\n",
    "                'exists': True,\n",
    "                'size_bytes': file_size,\n",
    "                'size_mb': file_size / (1024**2),\n",
    "                'size_gb': file_size / (1024**3),\n",
    "                'path': str(filepath)\n",
    "            }\n",
    "        else:\n",
    "            analysis['files'][name] = {\n",
    "                'exists': False,\n",
    "                'path': str(filepath)\n",
    "            }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Performing file system analysis\n",
    "fs_analysis = analyze_file_system()\n",
    "\n",
    "print(\"=== File System Analysis ===\")\n",
    "print(f\"Analysis timestamp: {fs_analysis['timestamp']}\")\n",
    "print(f\"\\nSystem Information:\")\n",
    "for key, value in fs_analysis['system_info'].items():\n",
    "    if 'gb' in key:\n",
    "        print(f\"  {key}: {value:.2f} GB\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nDataset Files:\")\n",
    "for name, info in fs_analysis['files'].items():\n",
    "    if info['exists']:\n",
    "        print(f\"  {name.upper()}:\")\n",
    "        print(f\"    Size: {info['size_gb']:.2f} GB ({info['size_mb']:.1f} MB)\")\n",
    "        print(f\"    Path: {info['path']}\")\n",
    "    else:\n",
    "        print(f\"  {name.upper()}: NOT FOUND at {info['path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78268510-1e46-421c-8739-ded870cf42ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Memory-Safe File Analysis ===\n",
      "Initial memory usage: 0.20 GB\n",
      "\n",
      "Analyzing CICFLOW file...\n",
      "  Rows: 3,540,241\n",
      "  Columns: 84\n",
      "  Estimated memory if fully loaded: 3.29 GB\n",
      "  ‚úì Safe to load fully (< 9.6 GB)\n",
      "\n",
      "Analyzing DATA file...\n",
      "  Rows: 447,915\n",
      "  Columns: 76\n",
      "  Estimated memory if fully loaded: 0.26 GB\n",
      "  ‚úì Safe to load fully (< 9.6 GB)\n",
      "\n",
      "Analyzing LABELS file...\n",
      "  Rows: 447,915\n",
      "  Columns: 1\n",
      "  Estimated memory if fully loaded: 0.01 GB\n",
      "  ‚úì Safe to load fully (< 9.6 GB)\n",
      "\n",
      "Final memory usage: 0.20 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Memory-Safe Data Loading Functions\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage\"\"\"\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    return {\n",
    "        'rss_gb': memory_info.rss / (1024**3),\n",
    "        'vms_gb': memory_info.vms / (1024**3),\n",
    "        'available_gb': psutil.virtual_memory().available / (1024**3)\n",
    "    }\n",
    "\n",
    "def load_file_info(filepath, nrows=5):\n",
    "    \"\"\"Load basic information about a CSV file without loading full data\"\"\"\n",
    "    try:\n",
    "        # Reading just the header and a few rows\n",
    "        sample_df = pd.read_csv(filepath, nrows=nrows, low_memory=False)\n",
    "        \n",
    "        # Getting total row count (memory efficient)\n",
    "        total_rows = sum(1 for line in open(filepath)) - 1  # Subtract header\n",
    "        \n",
    "        info = {\n",
    "            'columns': list(sample_df.columns),\n",
    "            'total_rows': total_rows,\n",
    "            'total_columns': len(sample_df.columns),\n",
    "            'dtypes': sample_df.dtypes.to_dict(),\n",
    "            'sample_data': sample_df.head(3).to_dict(),\n",
    "            'memory_usage_mb': 0,  # Will calculate later\n",
    "            'estimated_full_size_gb': 0  # Will estimate\n",
    "        }\n",
    "        \n",
    "        # Estimating memory usage\n",
    "        memory_per_row = sample_df.memory_usage(deep=True).sum() / len(sample_df)\n",
    "        estimated_total_memory = (memory_per_row * total_rows) / (1024**3)\n",
    "        info['estimated_full_size_gb'] = estimated_total_memory\n",
    "        \n",
    "        return info, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "print(\"=== Memory-Safe File Analysis ===\")\n",
    "print(f\"Initial memory usage: {get_memory_usage()['rss_gb']:.2f} GB\")\n",
    "\n",
    "# Analyzing each file\n",
    "file_info = {}\n",
    "for name, filepath in [('cicflow', CICFLOW_FILE), ('data', DATA_FILE), ('labels', LABEL_FILE)]:\n",
    "    if filepath.exists():\n",
    "        print(f\"\\nAnalyzing {name.upper()} file...\")\n",
    "        info, error = load_file_info(filepath)\n",
    "        \n",
    "        if error:\n",
    "            print(f\"  Error loading {name}: {error}\")\n",
    "            file_info[name] = {'error': error}\n",
    "        else:\n",
    "            file_info[name] = info\n",
    "            print(f\"  Rows: {info['total_rows']:,}\")\n",
    "            print(f\"  Columns: {info['total_columns']}\")\n",
    "            print(f\"  Estimated memory if fully loaded: {info['estimated_full_size_gb']:.2f} GB\")\n",
    "            \n",
    "            # Checking if safe to load fully\n",
    "            if info['estimated_full_size_gb'] < ANALYSIS_CONFIG['memory_limit_gb'] * 0.8:\n",
    "                print(f\"  ‚úì Safe to load fully (< {ANALYSIS_CONFIG['memory_limit_gb']*0.8:.1f} GB)\")\n",
    "            else:\n",
    "                print(f\"  ‚ö† Requires chunked processing (> {ANALYSIS_CONFIG['memory_limit_gb']*0.8:.1f} GB)\")\n",
    "    else:\n",
    "        print(f\"\\n{name.upper()} file not found!\")\n",
    "        file_info[name] = {'error': 'File not found'}\n",
    "\n",
    "print(f\"\\nFinal memory usage: {get_memory_usage()['rss_gb']:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60199dc-ad26-4078-a9e0-b0cfb65661f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CICFLOW STRUCTURE ANALYSIS ===\n",
      "Shape: (3540241, 84)\n",
      "Data types distribution:\n",
      "  float64: 45 columns\n",
      "  int64: 34 columns\n",
      "  object: 5 columns\n",
      "Missing values: 0\n",
      "Memory usage: 0.93 MB\n",
      "Categorical columns: 5\n",
      "  ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', 'Label']\n",
      "Numerical columns: 79\n",
      "\n",
      "=== DATA STRUCTURE ANALYSIS ===\n",
      "Shape: (447915, 76)\n",
      "Data types distribution:\n",
      "  float64: 45 columns\n",
      "  int64: 31 columns\n",
      "Missing values: 0\n",
      "Memory usage: 0.59 MB\n",
      "Numerical columns: 76\n",
      "\n",
      "=== LABELS STRUCTURE ANALYSIS ===\n",
      "Shape: (447915, 1)\n",
      "Data types distribution:\n",
      "  int64: 1 columns\n",
      "Missing values: 0\n",
      "Memory usage: 0.01 MB\n",
      "Numerical columns: 1\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Data Structure Analysis\n",
    "def analyze_data_structure():\n",
    "    \"\"\"Analyze the structure and basic properties of each dataset\"\"\"\n",
    "    \n",
    "    structure_analysis = {}\n",
    "    \n",
    "    for name, filepath in [('cicflow', CICFLOW_FILE), ('data', DATA_FILE), ('labels', LABEL_FILE)]:\n",
    "        if not filepath.exists():\n",
    "            structure_analysis[name] = {'error': 'File not found'}\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n=== {name.upper()} STRUCTURE ANALYSIS ===\")\n",
    "        \n",
    "        try:\n",
    "            # Loading sample for structure analysis\n",
    "            sample_df = pd.read_csv(filepath, nrows=1000, low_memory=False)\n",
    "            \n",
    "            analysis = {\n",
    "                'shape': (file_info[name]['total_rows'], file_info[name]['total_columns']),\n",
    "                'columns': list(sample_df.columns),\n",
    "                'data_types': {},\n",
    "                'missing_values': {},\n",
    "                'unique_values': {},\n",
    "                'memory_usage': {}\n",
    "            }\n",
    "            \n",
    "            # Analyzing each column\n",
    "            for col in sample_df.columns:\n",
    "                analysis['data_types'][col] = str(sample_df[col].dtype)\n",
    "                analysis['missing_values'][col] = sample_df[col].isnull().sum()\n",
    "                analysis['unique_values'][col] = sample_df[col].nunique()\n",
    "                analysis['memory_usage'][col] = sample_df[col].memory_usage(deep=True)\n",
    "            \n",
    "            # Summary statistics\n",
    "            analysis['summary'] = {\n",
    "                'total_missing': sum(analysis['missing_values'].values()),\n",
    "                'total_memory_mb': sum(analysis['memory_usage'].values()) / (1024**2),\n",
    "                'categorical_columns': [col for col, dtype in analysis['data_types'].items() \n",
    "                                      if dtype == 'object'],\n",
    "                'numerical_columns': [col for col, dtype in analysis['data_types'].items() \n",
    "                                    if dtype in ['int64', 'float64', 'int32', 'float32']],\n",
    "                'high_cardinality_columns': [col for col, unique_count in analysis['unique_values'].items()\n",
    "                                           if unique_count > len(sample_df) * 0.8]\n",
    "            }\n",
    "            \n",
    "            structure_analysis[name] = analysis\n",
    "            \n",
    "            # Displaying key information\n",
    "            print(f\"Shape: {analysis['shape']}\")\n",
    "            print(f\"Data types distribution:\")\n",
    "            dtype_counts = pd.Series(list(analysis['data_types'].values())).value_counts()\n",
    "            for dtype, count in dtype_counts.items():\n",
    "                print(f\"  {dtype}: {count} columns\")\n",
    "            \n",
    "            print(f\"Missing values: {analysis['summary']['total_missing']}\")\n",
    "            print(f\"Memory usage: {analysis['summary']['total_memory_mb']:.2f} MB\")\n",
    "            \n",
    "            if analysis['summary']['categorical_columns']:\n",
    "                print(f\"Categorical columns: {len(analysis['summary']['categorical_columns'])}\")\n",
    "                print(f\"  {analysis['summary']['categorical_columns'][:5]}{'...' if len(analysis['summary']['categorical_columns']) > 5 else ''}\")\n",
    "            \n",
    "            print(f\"Numerical columns: {len(analysis['summary']['numerical_columns'])}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {name}: {str(e)}\")\n",
    "            structure_analysis[name] = {'error': str(e)}\n",
    "    \n",
    "    return structure_analysis\n",
    "\n",
    "# Performing structure analysis\n",
    "structure_analysis = analyze_data_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "793cc723-0e47-4bf0-b374-25e37d4916c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COLUMN RELATIONSHIP ANALYSIS ===\n",
      "\n",
      "Common columns between CICFlow and Data: 76\n",
      "  Sample: ['Fwd Act Data Pkts', 'Fwd IAT Std', 'Bwd URG Flags', 'Fwd Packet Length Min', 'SYN Flag Count', 'FWD Init Win Bytes', 'Packet Length Variance', 'Flow IAT Max', 'Fwd IAT Total', 'Total Length of Bwd Packet']\n",
      "\n",
      "CICFlow unique columns: 8\n",
      "  Sample: ['Flow ID', 'Timestamp', 'Src Port', 'Dst IP', 'Label', 'Src IP', 'Dst Port', 'Protocol']\n",
      "\n",
      "Data unique columns: 0\n",
      "\n",
      "Label file columns: ['Label']\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Column Name Analysis and Comparison\n",
    "def analyze_column_relationships():\n",
    "    \"\"\"Analyze relationships between datasets based on column names\"\"\"\n",
    "    \n",
    "    print(\"\\n=== COLUMN RELATIONSHIP ANALYSIS ===\")\n",
    "    \n",
    "    # Extracting column names from each dataset\n",
    "    columns = {}\n",
    "    for name in ['cicflow', 'data', 'labels']:\n",
    "        if name in structure_analysis and 'columns' in structure_analysis[name]:\n",
    "            columns[name] = set(structure_analysis[name]['columns'])\n",
    "        else:\n",
    "            columns[name] = set()\n",
    "    \n",
    "    # Comparing column overlaps\n",
    "    if columns['cicflow'] and columns['data']:\n",
    "        common_cicflow_data = columns['cicflow'].intersection(columns['data'])\n",
    "        print(f\"\\nCommon columns between CICFlow and Data: {len(common_cicflow_data)}\")\n",
    "        if common_cicflow_data:\n",
    "            print(f\"  Sample: {list(common_cicflow_data)[:10]}\")\n",
    "        \n",
    "        cicflow_only = columns['cicflow'] - columns['data']\n",
    "        data_only = columns['data'] - columns['cicflow']\n",
    "        \n",
    "        print(f\"\\nCICFlow unique columns: {len(cicflow_only)}\")\n",
    "        if cicflow_only:\n",
    "            print(f\"  Sample: {list(cicflow_only)[:10]}\")\n",
    "        \n",
    "        print(f\"\\nData unique columns: {len(data_only)}\")\n",
    "        if data_only:\n",
    "            print(f\"  Sample: {list(data_only)[:10]}\")\n",
    "    \n",
    "    # Analyzing label columns\n",
    "    if columns['labels']:\n",
    "        print(f\"\\nLabel file columns: {list(columns['labels'])}\")\n",
    "        \n",
    "        # Checking if labels contain identifiers that might link to other files\n",
    "        label_columns = list(columns['labels'])\n",
    "        potential_ids = [col for col in label_columns if any(id_term in col.lower() \n",
    "                        for id_term in ['id', 'index', 'flow', 'record'])]\n",
    "        if potential_ids:\n",
    "            print(f\"Potential identifier columns in labels: {potential_ids}\")\n",
    "    \n",
    "    return columns\n",
    "\n",
    "# Analyzing column relationships\n",
    "column_relationships = analyze_column_relationships()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af450493-4d09-4efb-a7a2-0e35483b95c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAMPLE DATA INSPECTION ===\n",
      "\n",
      "--- CICFLOW SAMPLE DATA ---\n",
      "First 3 rows:\n",
      "                                    Flow ID        Src IP  Src Port          Dst IP  Dst Port  Protocol               Timestamp  Flow Duration  Total Fwd Packet  Total Bwd packets  Total Length of Fwd Packet  Total Length of Bwd Packet  Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  Fwd Packet Length Std  Bwd Packet Length Max  Bwd Packet Length Min  Bwd Packet Length Mean  Bwd Packet Length Std   Flow Bytes/s  Flow Packets/s  Flow IAT Mean   Flow IAT Std  Flow IAT Max  Flow IAT Min  Fwd IAT Total   Fwd IAT Mean    Fwd IAT Std  Fwd IAT Max  Fwd IAT Min  Bwd IAT Total  Bwd IAT Mean   Bwd IAT Std  Bwd IAT Max  Bwd IAT Min  Fwd PSH Flags  Bwd PSH Flags  Fwd URG Flags  Bwd URG Flags  Fwd Header Length  Bwd Header Length  Fwd Packets/s  Bwd Packets/s  Packet Length Min  Packet Length Max  Packet Length Mean  Packet Length Std  Packet Length Variance  FIN Flag Count  SYN Flag Count  RST Flag Count  PSH Flag Count  ACK Flag Count  URG Flag Count  CWR Flag Count  ECE Flag Count  Down/Up Ratio  Average Packet Size  Fwd Segment Size Avg  Bwd Segment Size Avg  Fwd Bytes/Bulk Avg  Fwd Packet/Bulk Avg  Fwd Bulk Rate Avg  Bwd Bytes/Bulk Avg  Bwd Packet/Bulk Avg  Bwd Bulk Rate Avg  Subflow Fwd Packets  Subflow Fwd Bytes  Subflow Bwd Packets  Subflow Bwd Bytes  FWD Init Win Bytes  Bwd Init Win Bytes  Fwd Act Data Pkts  Fwd Seg Size Min  Active Mean  Active Std  Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min           Label\n",
      "0    175.45.176.2-149.171.126.16-23357-80-6  175.45.176.2     23357  149.171.126.16        80         6  22/01/2015 07:50:15 AM         214392                 9                 21                       388.0                     24564.0                  194.0                    0.0               43.111111              85.545959                 1460.0                    0.0             1169.714286             552.156965  116384.939737      139.930594    7392.827586   17881.622845       61855.0           2.0       213501.0   26687.625000   35191.916072      87999.0         10.0       207850.0       10392.5  3.826029e+04     168673.0          2.0              0              0              0              0                212                436      41.979178      97.951416                0.0             1460.0          804.903226         702.892469           494057.823656               2               4               0               2              28               0               0               0            2.0           831.733333             43.111111           1169.714286                   0                    0                  0               24952                   20             402393                    0                  0                    0                  0               16383               16383                  2                20          0.0         0.0         0.0         0.0        0.0       0.0       0.0       0.0        Exploits\n",
      "1    175.45.176.0-149.171.126.16-13284-80-6  175.45.176.0     13284  149.171.126.16        80         6  22/01/2015 07:50:13 AM        2376792                 9                  3                       752.0                         0.0                  188.0                    0.0               83.555556              99.084700                    0.0                    0.0                0.000000               0.000000     316.392852        5.048822  216072.000000  638179.872769     2138664.0          11.0      2323484.0  290435.500000  747410.857535    2138664.0         11.0      2357699.0     1178849.5  1.667118e+06    2357680.0         19.0              0              0              0              0                212                 76       3.786617       1.262206                0.0              188.0           57.846154          90.312279             8156.307692               2               4               0               0              10               0               0               0            0.0            62.666667             83.555556              0.000000                   0                    0                  0                   0                    0                  0                    9                752                    3                  0               16383               16383                  4                20          0.0         0.0         0.0         0.0        0.0       0.0       0.0       0.0  Reconnaissance\n",
      "2  175.45.176.2-149.171.126.16-13792-5555-6  175.45.176.2     13792  149.171.126.16      5555         6  22/01/2015 07:50:16 AM         131350                10                  3                      7564.0                         0.0                 1460.0                    0.0              756.400000             690.497277                    0.0                    0.0                0.000000               0.000000   57586.600685       98.972212   10945.833333   21066.252205       62942.0           2.0       119039.0   13226.555556   25983.491245      62942.0          2.0       122595.0       61297.5  8.667503e+04     122586.0          9.0              0              0              0              0                232                 76      76.132470      22.839741                0.0             1460.0          540.285714         675.150516           455828.219780               1               4               0               0              11               0               0               0            0.0           581.846154            756.400000              0.000000                   0                    0                  0                7564                    6            6367003                    0                  0                    0                  0               16383               16383                  6                20          0.0         0.0         0.0         0.0        0.0       0.0       0.0       0.0        Exploits\n",
      "\n",
      "Column info:\n",
      "  Total columns: 84\n",
      "  Column names (first 10): ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets']\n",
      "  Potential label columns: ['Label']\n",
      "    Label: ['Exploits' 'Reconnaissance' 'DoS' 'Generic']\n",
      "\n",
      "--- DATA SAMPLE DATA ---\n",
      "First 3 rows:\n",
      "   Flow Duration  Total Fwd Packet  Total Bwd packets  Total Length of Fwd Packet  Total Length of Bwd Packet  Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  Fwd Packet Length Std  Bwd Packet Length Max  Bwd Packet Length Min  Bwd Packet Length Mean  Bwd Packet Length Std   Flow Bytes/s  Flow Packets/s  Flow IAT Mean   Flow IAT Std  Flow IAT Max  Flow IAT Min  Fwd IAT Total   Fwd IAT Mean    Fwd IAT Std  Fwd IAT Max  Fwd IAT Min  Bwd IAT Total  Bwd IAT Mean   Bwd IAT Std  Bwd IAT Max  Bwd IAT Min  Fwd PSH Flags  Bwd PSH Flags  Fwd URG Flags  Bwd URG Flags  Fwd Header Length  Bwd Header Length  Fwd Packets/s  Bwd Packets/s  Packet Length Min  Packet Length Max  Packet Length Mean  Packet Length Std  Packet Length Variance  FIN Flag Count  SYN Flag Count  RST Flag Count  PSH Flag Count  ACK Flag Count  URG Flag Count  CWR Flag Count  ECE Flag Count  Down/Up Ratio  Average Packet Size  Fwd Segment Size Avg  Bwd Segment Size Avg  Fwd Bytes/Bulk Avg  Fwd Packet/Bulk Avg  Fwd Bulk Rate Avg  Bwd Bytes/Bulk Avg  Bwd Packet/Bulk Avg  Bwd Bulk Rate Avg  Subflow Fwd Packets  Subflow Fwd Bytes  Subflow Bwd Packets  Subflow Bwd Bytes  FWD Init Win Bytes  Bwd Init Win Bytes  Fwd Act Data Pkts  Fwd Seg Size Min  Active Mean  Active Std  Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min\n",
      "0         214392                 9                 21                       388.0                     24564.0                  194.0                    0.0               43.111111              85.545959                 1460.0                    0.0             1169.714286             552.156965  116384.939737      139.930594    7392.827586   17881.622845       61855.0           2.0       213501.0   26687.625000   35191.916072      87999.0         10.0       207850.0       10392.5  3.826029e+04     168673.0          2.0              0              0              0              0                212                436      41.979178      97.951416                0.0             1460.0          804.903226         702.892469           494057.823656               2               4               0               2              28               0               0               0            2.0           831.733333             43.111111           1169.714286                   0                    0                  0               24952                   20             402393                    0                  0                    0                  0               16383               16383                  2                20          0.0         0.0         0.0         0.0        0.0       0.0       0.0       0.0\n",
      "1        2376792                 9                  3                       752.0                         0.0                  188.0                    0.0               83.555556              99.084700                    0.0                    0.0                0.000000               0.000000     316.392852        5.048822  216072.000000  638179.872769     2138664.0          11.0      2323484.0  290435.500000  747410.857535    2138664.0         11.0      2357699.0     1178849.5  1.667118e+06    2357680.0         19.0              0              0              0              0                212                 76       3.786617       1.262206                0.0              188.0           57.846154          90.312279             8156.307692               2               4               0               0              10               0               0               0            0.0            62.666667             83.555556              0.000000                   0                    0                  0                   0                    0                  0                    9                752                    3                  0               16383               16383                  4                20          0.0         0.0         0.0         0.0        0.0       0.0       0.0       0.0\n",
      "2         131350                10                  3                      7564.0                         0.0                 1460.0                    0.0              756.400000             690.497277                    0.0                    0.0                0.000000               0.000000   57586.600685       98.972212   10945.833333   21066.252205       62942.0           2.0       119039.0   13226.555556   25983.491245      62942.0          2.0       122595.0       61297.5  8.667503e+04     122586.0          9.0              0              0              0              0                232                 76      76.132470      22.839741                0.0             1460.0          540.285714         675.150516           455828.219780               1               4               0               0              11               0               0               0            0.0           581.846154            756.400000              0.000000                   0                    0                  0                7564                    6            6367003                    0                  0                    0                  0               16383               16383                  6                20          0.0         0.0         0.0         0.0        0.0       0.0       0.0       0.0\n",
      "\n",
      "Column info:\n",
      "  Total columns: 76\n",
      "  Column names (first 10): ['Flow Duration', 'Total Fwd Packet', 'Total Bwd packets', 'Total Length of Fwd Packet', 'Total Length of Bwd Packet', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max']\n",
      "\n",
      "--- LABELS SAMPLE DATA ---\n",
      "First 3 rows:\n",
      "   Label\n",
      "0      4\n",
      "1      7\n",
      "2      4\n",
      "\n",
      "Column info:\n",
      "  Total columns: 1\n",
      "  Column names (first 10): ['Label']\n",
      "  Potential label columns: ['Label']\n",
      "    Label: [4 7 3 6]\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Sample Data Inspection\n",
    "def inspect_sample_data():\n",
    "    \"\"\"Inspect sample data from each file to understand content and format\"\"\"\n",
    "    \n",
    "    print(\"\\n=== SAMPLE DATA INSPECTION ===\")\n",
    "    \n",
    "    sample_data = {}\n",
    "    \n",
    "    for name, filepath in [('cicflow', CICFLOW_FILE), ('data', DATA_FILE), ('labels', LABEL_FILE)]:\n",
    "        if not filepath.exists():\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- {name.upper()} SAMPLE DATA ---\")\n",
    "        \n",
    "        try:\n",
    "            # Loading small sample\n",
    "            df_sample = pd.read_csv(filepath, nrows=5, low_memory=False)\n",
    "            sample_data[name] = df_sample\n",
    "            \n",
    "            print(f\"First 3 rows:\")\n",
    "            print(df_sample.head(3).to_string())\n",
    "            \n",
    "            print(f\"\\nColumn info:\")\n",
    "            print(f\"  Total columns: {len(df_sample.columns)}\")\n",
    "            print(f\"  Column names (first 10): {list(df_sample.columns)[:10]}\")\n",
    "            \n",
    "            # Checking for potential label/target columns\n",
    "            potential_labels = [col for col in df_sample.columns \n",
    "                              if any(term in col.lower() for term in ['label', 'attack', 'class', 'target'])]\n",
    "            if potential_labels:\n",
    "                print(f\"  Potential label columns: {potential_labels}\")\n",
    "                for label_col in potential_labels:\n",
    "                    unique_vals = df_sample[label_col].unique()\n",
    "                    print(f\"    {label_col}: {unique_vals}\")\n",
    "            \n",
    "            # Checking for missing values in sample\n",
    "            missing_in_sample = df_sample.isnull().sum()\n",
    "            if missing_in_sample.sum() > 0:\n",
    "                print(f\"  Missing values in sample: {missing_in_sample[missing_in_sample > 0].to_dict()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample from {name}: {str(e)}\")\n",
    "            sample_data[name] = None\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "# Inspecting sample data\n",
    "sample_data = inspect_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1020da1e-cd40-4b14-9317-a724bf1c0272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA CONSISTENCY VALIDATION ===\n",
      "\n",
      "--- ROW COUNT ANALYSIS ---\n",
      "CICFLOW: 3,540,241 rows\n",
      "DATA: 447,915 rows\n",
      "LABELS: 447,915 rows\n",
      "\n",
      "--- RELATIONSHIP ANALYSIS ---\n",
      "‚úì Data.csv and Label.csv have matching row counts - likely aligned\n",
      "üìä CICFlow has more rows than Data - unexpected pattern\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Data Consistency Validation\n",
    "def validate_data_consistency():\n",
    "    \"\"\"Validate consistency between the three datasets\"\"\"\n",
    "    \n",
    "    print(\"\\n=== DATA CONSISTENCY VALIDATION ===\")\n",
    "    \n",
    "    validation_results = {\n",
    "        'row_count_analysis': {},\n",
    "        'potential_relationships': {},\n",
    "        'data_alignment': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Row count analysis\n",
    "    print(\"\\n--- ROW COUNT ANALYSIS ---\")\n",
    "    row_counts = {}\n",
    "    for name in ['cicflow', 'data', 'labels']:\n",
    "        if name in file_info and 'total_rows' in file_info[name]:\n",
    "            row_counts[name] = file_info[name]['total_rows']\n",
    "            print(f\"{name.upper()}: {row_counts[name]:,} rows\")\n",
    "    \n",
    "    validation_results['row_count_analysis'] = row_counts\n",
    "    \n",
    "    # Analyzing potential relationships\n",
    "    print(\"\\n--- RELATIONSHIP ANALYSIS ---\")\n",
    "    if 'data' in row_counts and 'labels' in row_counts:\n",
    "        if row_counts['data'] == row_counts['labels']:\n",
    "            print(\"‚úì Data.csv and Label.csv have matching row counts - likely aligned\")\n",
    "            validation_results['potential_relationships']['data_labels'] = 'aligned'\n",
    "            validation_results['recommendations'].append(\"Data.csv and Label.csv appear to be aligned by row index\")\n",
    "        else:\n",
    "            print(\"‚ö† Data.csv and Label.csv have different row counts\")\n",
    "            validation_results['potential_relationships']['data_labels'] = 'misaligned'\n",
    "            validation_results['recommendations'].append(\"Investigate row count mismatch between Data.csv and Label.csv\")\n",
    "    \n",
    "    if 'cicflow' in row_counts:\n",
    "        cicflow_rows = row_counts['cicflow']\n",
    "        data_rows = row_counts.get('data', 0)\n",
    "        \n",
    "        if cicflow_rows < data_rows:\n",
    "            ratio = data_rows / cicflow_rows if cicflow_rows > 0 else 0\n",
    "            print(f\"üìä CICFlow has fewer rows than Data ({ratio:.1f}x difference)\")\n",
    "            print(\"   This suggests CICFlow contains aggregated flow data while Data contains individual records\")\n",
    "            validation_results['potential_relationships']['cicflow_data'] = 'hierarchical'\n",
    "            validation_results['recommendations'].append(\"CICFlow appears to contain aggregated flows - investigate packet-to-flow mapping\")\n",
    "        elif cicflow_rows > data_rows:\n",
    "            print(\"üìä CICFlow has more rows than Data - unexpected pattern\")\n",
    "            validation_results['potential_relationships']['cicflow_data'] = 'unexpected'\n",
    "        else:\n",
    "            print(\"üìä CICFlow and Data have same row count - might be different feature sets of same data\")\n",
    "            validation_results['potential_relationships']['cicflow_data'] = 'same_level'\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validating data consistency\n",
    "validation_results = validate_data_consistency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb923f4-6bfc-4594-bb8b-c1fc518b4ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MEMORY USAGE AND LOADING STRATEGY ===\n",
      "Available memory: 6.75 GB\n",
      "Memory limit for analysis: 12.00 GB\n",
      "\n",
      "CICFLOW:\n",
      "  Estimated memory: 3.29 GB\n",
      "  ‚úì Strategy: Full load in memory\n",
      "\n",
      "DATA:\n",
      "  Estimated memory: 0.26 GB\n",
      "  ‚úì Strategy: Full load in memory\n",
      "\n",
      "LABELS:\n",
      "  Estimated memory: 0.01 GB\n",
      "  ‚úì Strategy: Full load in memory\n",
      "\n",
      "Total estimated memory for all files: 3.57 GB\n",
      "‚úì Recommendation: Can load multiple files if needed\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Memory Usage and Loading Strategy Analysis\n",
    "def analyze_memory_strategy():\n",
    "    \"\"\"Analyze memory requirements and recommend loading strategies\"\"\"\n",
    "    \n",
    "    print(\"\\n=== MEMORY USAGE AND LOADING STRATEGY ===\")\n",
    "    \n",
    "    memory_analysis = {\n",
    "        'current_usage': get_memory_usage(),\n",
    "        'file_requirements': {},\n",
    "        'loading_strategies': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    available_memory = memory_analysis['current_usage']['available_gb']\n",
    "    memory_limit = ANALYSIS_CONFIG['memory_limit_gb']\n",
    "    \n",
    "    print(f\"Available memory: {available_memory:.2f} GB\")\n",
    "    print(f\"Memory limit for analysis: {memory_limit:.2f} GB\")\n",
    "    \n",
    "    # Analyzing each file's memory requirements\n",
    "    for name in ['cicflow', 'data', 'labels']:\n",
    "        if name in file_info and 'estimated_full_size_gb' in file_info[name]:\n",
    "            estimated_size = file_info[name]['estimated_full_size_gb']\n",
    "            memory_analysis['file_requirements'][name] = estimated_size\n",
    "            \n",
    "            print(f\"\\n{name.upper()}:\")\n",
    "            print(f\"  Estimated memory: {estimated_size:.2f} GB\")\n",
    "            \n",
    "            if estimated_size < memory_limit * 0.6:  # Conservative threshold\n",
    "                strategy = \"full_load\"\n",
    "                print(f\"  ‚úì Strategy: Full load in memory\")\n",
    "            elif estimated_size < memory_limit * 0.9:\n",
    "                strategy = \"careful_load\"\n",
    "                print(f\"  ‚ö† Strategy: Careful loading with monitoring\")\n",
    "            else:\n",
    "                strategy = \"chunked_processing\"\n",
    "                print(f\"  üîÑ Strategy: Chunked processing required\")\n",
    "                chunk_size = max(1000, int(memory_limit * 0.3 * 1024**3 / (estimated_size * 1024**3 / file_info[name]['total_rows'])))\n",
    "                print(f\"    Recommended chunk size: {chunk_size:,} rows\")\n",
    "            \n",
    "            memory_analysis['loading_strategies'][name] = {\n",
    "                'strategy': strategy,\n",
    "                'estimated_size_gb': estimated_size,\n",
    "                'chunk_size': chunk_size if strategy == 'chunked_processing' else None\n",
    "            }\n",
    "    \n",
    "    # Generating recommendations\n",
    "    total_estimated = sum(memory_analysis['file_requirements'].values())\n",
    "    print(f\"\\nTotal estimated memory for all files: {total_estimated:.2f} GB\")\n",
    "    \n",
    "    if total_estimated > memory_limit:\n",
    "        memory_analysis['recommendations'].append(\"Cannot load all files simultaneously - process sequentially\")\n",
    "        print(\"‚ö† Recommendation: Process files sequentially\")\n",
    "    else:\n",
    "        memory_analysis['recommendations'].append(\"Can potentially load multiple files simultaneously\")\n",
    "        print(\"‚úì Recommendation: Can load multiple files if needed\")\n",
    "    \n",
    "    return memory_analysis\n",
    "\n",
    "# Analyzing memory strategy\n",
    "memory_strategy = analyze_memory_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc433a84-7c03-4875-82dc-48be6b2e92f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "           COMPREHENSIVE DATA EXPLORATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìä DATASET OVERVIEW\n",
      "------------------------------\n",
      "Total data points across files: 4,436,071\n",
      "Total features: 160\n",
      "Successfully analyzed files: 3/3\n",
      "Estimated total size: 3.57 GB\n",
      "\n",
      "‚öôÔ∏è TECHNICAL SPECIFICATIONS\n",
      "------------------------------\n",
      "CICFLOW: full_load (3.29 GB)\n",
      "DATA: full_load (0.26 GB)\n",
      "LABELS: full_load (0.01 GB)\n",
      "\n",
      "üîó DATA RELATIONSHIPS\n",
      "------------------------------\n",
      "data_labels: aligned\n",
      "cicflow_data: unexpected\n",
      "\n",
      "üí° PROCESSING RECOMMENDATIONS\n",
      "------------------------------\n",
      "1. Data.csv and Label.csv appear to be aligned by row index\n",
      "2. Can potentially load multiple files simultaneously\n",
      "\n",
      "üöÄ RECOMMENDED NEXT STEPS\n",
      "------------------------------\n",
      "1. Implement chunked data loading for CICFlowMeter.csv\n",
      "2. Analyze label distribution and class imbalance\n",
      "3. Investigate feature correlations and engineering opportunities\n",
      "4. Design multi-modal feature separation strategy\n",
      "5. Create graph construction pipeline for GraphVAE\n",
      "6. Develop memory-efficient preprocessing pipeline\n",
      "\n",
      "üíæ Summary report saved to: C:\\Users\\Ashutosh\\Documents\\Projects\\NIDS\\results\\data_exploration\\data_exploration_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Generate Comprehensive Summary Report\n",
    "def generate_exploration_summary():\n",
    "    \"\"\"Generate a comprehensive summary of the data exploration\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"           COMPREHENSIVE DATA EXPLORATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    summary_report = {\n",
    "        'analysis_timestamp': datetime.now().isoformat(),\n",
    "        'dataset_overview': {},\n",
    "        'technical_specifications': {},\n",
    "        'data_relationships': {},\n",
    "        'processing_recommendations': {},\n",
    "        'next_steps': []\n",
    "    }\n",
    "    \n",
    "    # Dataset Overview\n",
    "    print(\"\\nüìä DATASET OVERVIEW\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    total_rows = sum(file_info[name].get('total_rows', 0) for name in ['cicflow', 'data', 'labels'] \n",
    "                    if name in file_info and 'total_rows' in file_info[name])\n",
    "    total_features = sum(file_info[name].get('total_columns', 0) for name in ['cicflow', 'data'] \n",
    "                        if name in file_info and 'total_columns' in file_info[name])\n",
    "    \n",
    "    summary_report['dataset_overview'] = {\n",
    "        'total_data_points': total_rows,\n",
    "        'total_features': total_features,\n",
    "        'files_analyzed': len([name for name in file_info if 'error' not in file_info[name]]),\n",
    "        'estimated_total_size_gb': sum(file_info[name].get('estimated_full_size_gb', 0) \n",
    "                                     for name in file_info if 'estimated_full_size_gb' in file_info[name])\n",
    "    }\n",
    "    \n",
    "    print(f\"Total data points across files: {total_rows:,}\")\n",
    "    print(f\"Total features: {total_features}\")\n",
    "    print(f\"Successfully analyzed files: {summary_report['dataset_overview']['files_analyzed']}/3\")\n",
    "    print(f\"Estimated total size: {summary_report['dataset_overview']['estimated_total_size_gb']:.2f} GB\")\n",
    "    \n",
    "    # Technical Specifications\n",
    "    print(f\"\\n‚öôÔ∏è TECHNICAL SPECIFICATIONS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    summary_report['technical_specifications'] = {\n",
    "        'hardware_constraints': {\n",
    "            'available_memory_gb': memory_strategy['current_usage']['available_gb'],\n",
    "            'memory_limit_gb': ANALYSIS_CONFIG['memory_limit_gb']\n",
    "        },\n",
    "        'processing_requirements': memory_strategy['loading_strategies']\n",
    "    }\n",
    "    \n",
    "    for name, strategy_info in memory_strategy['loading_strategies'].items():\n",
    "        print(f\"{name.upper()}: {strategy_info['strategy']} ({strategy_info['estimated_size_gb']:.2f} GB)\")\n",
    "    \n",
    "    # Data Relationships\n",
    "    print(f\"\\nüîó DATA RELATIONSHIPS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    summary_report['data_relationships'] = validation_results['potential_relationships']\n",
    "    \n",
    "    for relationship, status in validation_results['potential_relationships'].items():\n",
    "        print(f\"{relationship}: {status}\")\n",
    "    \n",
    "    # Processing Recommendations\n",
    "    print(f\"\\nüí° PROCESSING RECOMMENDATIONS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    all_recommendations = (validation_results['recommendations'] + \n",
    "                         memory_strategy['recommendations'])\n",
    "    \n",
    "    summary_report['processing_recommendations'] = all_recommendations\n",
    "    \n",
    "    for i, rec in enumerate(all_recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    # Next Steps\n",
    "    print(f\"\\nüöÄ RECOMMENDED NEXT STEPS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    next_steps = [\n",
    "        \"Implement chunked data loading for CICFlowMeter.csv\",\n",
    "        \"Analyze label distribution and class imbalance\",\n",
    "        \"Investigate feature correlations and engineering opportunities\",\n",
    "        \"Design multi-modal feature separation strategy\",\n",
    "        \"Create graph construction pipeline for GraphVAE\",\n",
    "        \"Develop memory-efficient preprocessing pipeline\"\n",
    "    ]\n",
    "    \n",
    "    summary_report['next_steps'] = next_steps\n",
    "    \n",
    "    for i, step in enumerate(next_steps, 1):\n",
    "        print(f\"{i}. {step}\")\n",
    "    \n",
    "    # Save summary report\n",
    "    report_path = RESULTS_PATH / \"data_exploration_summary.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nüíæ Summary report saved to: {report_path}\")\n",
    "    \n",
    "    return summary_report\n",
    "\n",
    "# Generate comprehensive summary\n",
    "final_summary = generate_exploration_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a97be374-cbb5-4d5a-ac8a-6d5beaf70ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPORTING ANALYSIS RESULTS ===\n",
      "‚úÖ Complete analysis exported to: C:\\Users\\Ashutosh\\Documents\\Projects\\NIDS\\results\\data_exploration\\complete_data_exploration.json\n",
      "üìù Quick reference guide saved to: C:\\Users\\Ashutosh\\Documents\\Projects\\NIDS\\results\\data_exploration\\quick_reference.md\n",
      "\n",
      "üéØ Data exploration complete! Ready for feature analysis phase.\n",
      "Total analysis time: 2025-09-28 15:44:09.730579\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Export Analysis Results\n",
    "# Saving all analysis results for future reference\n",
    "print(\"\\n=== EXPORTING ANALYSIS RESULTS ===\")\n",
    "\n",
    "# Creating comprehensive analysis export\n",
    "analysis_export = {\n",
    "    'metadata': {\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'analyst': 'Bachelor Student',\n",
    "        'project': 'NIDS-Bachelor-Project',\n",
    "        'analysis_version': '1.0'\n",
    "    },\n",
    "    'file_system_analysis': fs_analysis,\n",
    "    'file_information': file_info,\n",
    "    'structure_analysis': structure_analysis,\n",
    "    'column_relationships': column_relationships,\n",
    "    'validation_results': validation_results,\n",
    "    'memory_strategy': memory_strategy,\n",
    "    'summary_report': final_summary\n",
    "}\n",
    "\n",
    "# Exporting to JSON\n",
    "export_path = RESULTS_PATH / \"complete_data_exploration.json\"\n",
    "with open(export_path, 'w') as f:\n",
    "    json.dump(analysis_export, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Complete analysis exported to: {export_path}\")\n",
    "\n",
    "# Creating a quick reference guide\n",
    "quick_ref = f\"\"\"\n",
    "# CIC-UNSW-NB15 Dataset - Quick Reference Guide\n",
    "\n",
    "## Dataset Files\n",
    "- CICFlowMeter_out.csv: {file_info.get('cicflow', {}).get('total_rows', 'Unknown'):,} rows, {file_info.get('cicflow', {}).get('total_columns', 'Unknown')} columns\n",
    "- Data.csv: {file_info.get('data', {}).get('total_rows', 'Unknown'):,} rows, {file_info.get('data', {}).get('total_columns', 'Unknown')} columns  \n",
    "- Label.csv: {file_info.get('labels', {}).get('total_rows', 'Unknown'):,} rows, {file_info.get('labels', {}).get('total_columns', 'Unknown')} columns\n",
    "\n",
    "## Processing Strategy\n",
    "- CICFlowMeter: {memory_strategy['loading_strategies'].get('cicflow', {}).get('strategy', 'Unknown')}\n",
    "- Data: {memory_strategy['loading_strategies'].get('data', {}).get('strategy', 'Unknown')}\n",
    "- Labels: {memory_strategy['loading_strategies'].get('labels', {}).get('strategy', 'Unknown')}\n",
    "\n",
    "## Key Findings\n",
    "{chr(10).join([f\"- {rec}\" for rec in validation_results['recommendations']])}\n",
    "\n",
    "## Next Phase: Feature Analysis (02_feature_analysis.ipynb)\n",
    "\"\"\"\n",
    "\n",
    "quick_ref_path = RESULTS_PATH / \"quick_reference.md\"\n",
    "with open(quick_ref_path, 'w') as f:\n",
    "    f.write(quick_ref)\n",
    "\n",
    "print(f\"üìù Quick reference guide saved to: {quick_ref_path}\")\n",
    "print(f\"\\nüéØ Data exploration complete! Ready for feature analysis phase.\")\n",
    "print(f\"Total analysis time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15caac5e-ed17-436d-af58-03660b5da742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
